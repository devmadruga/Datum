{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbef3ba3-47c0-4205-ab47-2dd6442aef1e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa4c7e21-7461-4e59-b9f1-e7dd25d93aa8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18087fa6-a095-4bb2-8be7-7935d1122221",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# 2. Kaggle Autenticação\n",
    "\n",
    "> Teste Prático -> Escolha um conjunto de dados do Kaggle relacionado a vendas. Certifique-se de que o conjunto de dados inclui informações como datas, produtos, quantidades vendidas, etc.\n",
    "\n",
    "> Teste Prático -> Carregue o conjunto de dados no Databricks.\n",
    "\n",
    "Devido a pequena quantidade de dados deste DataSet, preferi colocar os dados no próprio dbfs. Em outras situações, com grande volumetria de dados, poderia ter utilizado um serviço de storage em cloud mais adequado [eg: Blob Storage (Azure) ou S3 (AWS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "885514df-b210-4026-b8e4-683e5810a3b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "json_file_path = 'dbfs:/FileStore/KaggleToken/kaggle.json'\n",
    "\n",
    "spark_json_df = spark.read.format('json').option('header', 'true').option('inferschema', 'true').load(json_file_path)\n",
    "\n",
    "KAGGLE_USERNAME = spark_json_df.select(spark_json_df.username).take(1)[0]['username']\n",
    "KAGGLE_KEY= spark_json_df.select(spark_json_df.key).take(1)[0]['key']\n",
    "\n",
    "KAGGLE_DATASET_NAME = \"olistbr/brazilian-ecommerce\"\n",
    "DBFS_DEST_PATH = \"dbfs:/FileStore/Datum/KaggleOlistData/bronze\"\n",
    "DBFS_LAST_UPDATE_OLIST_DATASET = \"dbfs:/FileStore/Datum/KaggleOlistLastUpdate/last_update.txt\"\n",
    "PREVIOUS_UPDATE = datetime.datetime(2020, 10, 1, 19, 8, 27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f79418aa-7be6-4f97-839b-6bd354a5b85f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def kaggle_auth(KAGGLE_USERNAME, KAGGLE_KEY):\n",
    "    os.environ[\"KAGGLE_USERNAME\"] = KAGGLE_USERNAME\n",
    "    os.environ[\"KAGGLE_KEY\"] = KAGGLE_KEY\n",
    "\n",
    "    from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "    \n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "\n",
    "    print(\"Auth success\")\n",
    "\n",
    "    return api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3092a6ce-d2b9-4c24-987b-4aa2f21c8920",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module named 'kaggle'\nInstalling kaggle module...\nCollecting kaggle\n  Using cached kaggle-1.6.3-py3-none-any.whl\nRequirement already satisfied: urllib3 in /databricks/python3/lib/python3.10/site-packages (from kaggle) (1.26.14)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.10/site-packages (from kaggle) (2.28.1)\nRequirement already satisfied: bleach in /databricks/python3/lib/python3.10/site-packages (from kaggle) (4.1.0)\nRequirement already satisfied: certifi in /databricks/python3/lib/python3.10/site-packages (from kaggle) (2022.12.7)\nCollecting tqdm\n  Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)\nRequirement already satisfied: six>=1.10 in /usr/lib/python3/dist-packages (from kaggle) (1.16.0)\nRequirement already satisfied: python-dateutil in /databricks/python3/lib/python3.10/site-packages (from kaggle) (2.8.2)\nCollecting python-slugify\n  Using cached python_slugify-8.0.2-py2.py3-none-any.whl (10 kB)\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.10/site-packages (from bleach->kaggle) (22.0)\nRequirement already satisfied: webencodings in /databricks/python3/lib/python3.10/site-packages (from bleach->kaggle) (0.5.1)\nCollecting text-unidecode>=1.3\n  Using cached text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.10/site-packages (from requests->kaggle) (3.4)\nRequirement already satisfied: charset-normalizer<3,>=2 in /databricks/python3/lib/python3.10/site-packages (from requests->kaggle) (2.0.4)\nInstalling collected packages: text-unidecode, tqdm, python-slugify, kaggle\nSuccessfully installed kaggle-1.6.3 python-slugify-8.0.2 text-unidecode-1.3 tqdm-4.66.1\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n[notice] A new release of pip available: 22.3.1 -> 23.3.2\n[notice] To update, run: pip install --upgrade pip\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auth success\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    api = kaggle_auth(KAGGLE_USERNAME, KAGGLE_KEY)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(\"Installing kaggle module...\")\n",
    "\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"kaggle\"])\n",
    "    api = kaggle_auth(KAGGLE_USERNAME, KAGGLE_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0e945b0-fc05-45b1-8462-4a165cc6a06d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "del spark_json_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbff1071-b163-4f57-9158-1baba8498891",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# 3. Funções Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69b0f4c8-b21a-4f7a-a447-83b5fbc16ef7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def obter_informacoes_dataset(api, nome_completo_dataset):\n",
    "    try:\n",
    "        datasets = api.dataset_list(search=nome_completo_dataset)\n",
    "        for dataset in datasets:\n",
    "            if dataset.ref == nome_completo_dataset:\n",
    "                return dataset\n",
    "        print(f\"Dataset {nome_completo_dataset} não encontrado.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao recuperar informações do dataset: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d888402-bafa-448a-abe3-de40262aee16",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def verificar_existencia_arquivos(caminho_dbfs):\n",
    "    try:\n",
    "        if dbutils.fs.ls(caminho_dbfs):\n",
    "            print(f'--> Camada bronze \"{caminho_dbfs}\" existe.')\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        print(f'--> Camada bronze \"{caminho_dbfs}\" não existe.')\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c408e7a-5502-476a-83ce-aafc4442496c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def salvar_data_ultima_atualizacao(caminho_arquivo, data_atualizacao):\n",
    "    caminho = caminho_arquivo.replace(\"dbfs:/\", \"/dbfs/\")\n",
    "    with open(caminho, \"w\") as arquivo:\n",
    "        arquivo.write(data_atualizacao.strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61020b9b-f1df-497f-b974-62a22c37e307",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def ler_data_atualizacao(caminho_arquivo, data_padrao):\n",
    "    caminho = caminho_arquivo.replace(\"dbfs:/\", \"/dbfs/\")\n",
    "    try:\n",
    "        with open(caminho, \"r\") as arquivo:\n",
    "            data_str = arquivo.read()\n",
    "            print(\n",
    "                f'--> Watermark existe e possui valor {datetime.datetime.strptime(data_str, \"%Y-%m-%d %H:%M:%S\")}.'\n",
    "            )\n",
    "            return datetime.datetime.strptime(data_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            f'--> Watermark com última atualização do DataSet da Olist no Kaggle não encontrado em \"{caminho}\". Novo watermark será criado com o datetime {PREVIOUS_UPDATE}.'\n",
    "        )\n",
    "        salvar_data_ultima_atualizacao(caminho_arquivo, data_padrao)\n",
    "        return data_padrao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8e11c24-ffcf-4a75-be61-d15c3171ffbf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def baixar_e_salvar_dataset(api, nome_dataset, caminho_dbfs):\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        api.dataset_download_files(nome_dataset, path=tmpdir, unzip=True)\n",
    "\n",
    "        arquivos = os.listdir(tmpdir)\n",
    "        for arquivo in arquivos:\n",
    "            caminho_arquivo = os.path.join(tmpdir, arquivo)\n",
    "            caminho_destino = os.path.join(caminho_dbfs, arquivo)\n",
    "            dbutils.fs.cp(f\"file:{caminho_arquivo}\", caminho_destino)\n",
    "            print(f\"* Arquivo {arquivo} salvo em {caminho_destino}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60b84495-3f34-465b-8fd4-1d5ff3a91dd5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def verificar_atualizacao_e_download(api, nome_dataset, caminho_dbfs, data_anterior):\n",
    "    dataset = obter_informacoes_dataset(api, nome_dataset)\n",
    "\n",
    "    if dataset:\n",
    "        data_ultima_atualizacao = dataset.lastUpdated\n",
    "        arquivos_existem = verificar_existencia_arquivos(caminho_dbfs)\n",
    "\n",
    "        data_anterior = ler_data_atualizacao(\n",
    "            DBFS_LAST_UPDATE_OLIST_DATASET, data_anterior\n",
    "        )\n",
    "        necessita_atualizacao = data_ultima_atualizacao > data_anterior\n",
    "\n",
    "        if necessita_atualizacao or not arquivos_existem:\n",
    "            if necessita_atualizacao:\n",
    "                print(\n",
    "                    f\"--> Dataset atualizado após {data_anterior}, iniciando download...\"\n",
    "                )\n",
    "            else:\n",
    "                print(\n",
    "                    f\"--> Iniciando download dos arquivos do DataSet {KAGGLE_DATASET_NAME}...\"\n",
    "                )\n",
    "            baixar_e_salvar_dataset(api, nome_dataset, caminho_dbfs)\n",
    "\n",
    "            salvar_data_ultima_atualizacao(\n",
    "                DBFS_LAST_UPDATE_OLIST_DATASET, data_ultima_atualizacao\n",
    "            )\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"--> Dataset está atualizado e todos os arquivos da bronze já existem no DBFS. Esta Exception é customizada para parar o Workflow.\"\n",
    "            )\n",
    "    else:\n",
    "        print(\"--> Não foi possível obter as informações do dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "859dba42-46ff-48c5-a311-32ec6fbe1545",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# 4. Verificando Atualizações\n",
    "\n",
    "Se houver atualização no dataset, faremos o download dos novos arquivos e atualizaremos a watermark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2212d3d-265f-4131-90fb-fbe3ebf55031",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Camada bronze \"dbfs:/FileStore/Datum/KaggleOlistData/bronze\" não existe.\n--> Watermark existe e possui valor 2021-10-01 19:08:27.\n--> Iniciando download dos arquivos do DataSet olistbr/brazilian-ecommerce...\n* Arquivo olist_sellers_dataset.csv salvo em dbfs:/FileStore/Datum/KaggleOlistData/bronze/olist_sellers_dataset.csv\n* Arquivo olist_geolocation_dataset.csv salvo em dbfs:/FileStore/Datum/KaggleOlistData/bronze/olist_geolocation_dataset.csv\n* Arquivo olist_products_dataset.csv salvo em dbfs:/FileStore/Datum/KaggleOlistData/bronze/olist_products_dataset.csv\n* Arquivo olist_order_items_dataset.csv salvo em dbfs:/FileStore/Datum/KaggleOlistData/bronze/olist_order_items_dataset.csv\n* Arquivo olist_orders_dataset.csv salvo em dbfs:/FileStore/Datum/KaggleOlistData/bronze/olist_orders_dataset.csv\n* Arquivo olist_customers_dataset.csv salvo em dbfs:/FileStore/Datum/KaggleOlistData/bronze/olist_customers_dataset.csv\n* Arquivo olist_order_reviews_dataset.csv salvo em dbfs:/FileStore/Datum/KaggleOlistData/bronze/olist_order_reviews_dataset.csv\n* Arquivo olist_order_payments_dataset.csv salvo em dbfs:/FileStore/Datum/KaggleOlistData/bronze/olist_order_payments_dataset.csv\n* Arquivo product_category_name_translation.csv salvo em dbfs:/FileStore/Datum/KaggleOlistData/bronze/product_category_name_translation.csv\n"
     ]
    }
   ],
   "source": [
    "verificar_atualizacao_e_download(api, KAGGLE_DATASET_NAME, DBFS_DEST_PATH, PREVIOUS_UPDATE)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4394715856540342,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "KaggleAutenticacao_IngestaoDados",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
